FROM bolabaseten/flash-attn2:2.0.0.post1
# For MLPerf
 
RUN pip install transformers --upgrade
RUN pip install flash-attn==2.2.0 torch==2.2.0 datasets
# Install FlashAttention
# RUN pip install flash-attn==2.5.9.post1

# # Install CUDA extensions for fused dense
# RUN pip install git+https://github.com/HazyResearch/flash-attention@v2.5.9.post1#subdirectory=csrc/fused_dense_lib
